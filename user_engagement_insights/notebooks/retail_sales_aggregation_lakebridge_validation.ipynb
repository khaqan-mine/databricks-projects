{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "675747bd-1564-4428-9238-791b8150b27e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_raw = spark.read.table(\"retail_agg_ff\")\n",
    "df_raw.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88a66a49-c4fe-4c5b-9616-3cc5bb2d5084",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Start from the existing table\n",
    "# This reads the source data from the retail_agg_ff table, which contains raw retail transactions.\n",
    "from pyspark.sql.functions import col, last, sum, count, monotonically_increasing_id, lit\n",
    "from pyspark.sql.types import StringType, LongType\n",
    "\n",
    "df_raw = spark.read.table(\"retail_agg_ff\")\n",
    "\n",
    "# Sorter node (descending by STORE_ID and ITEM_DESC)\n",
    "srt_storeid = df_raw.orderBy(col('STORE_ID').desc(), col('ITEM_DESC').desc())\n",
    "\n",
    "# Aggregator: Single Agg\n",
    "aggtrans_single_agg = srt_storeid.groupBy(\"STORE_ID\", \"ITEM_DESC\").agg(\n",
    "    last(\"QTY\", ignorenulls=True).alias(\"QTY\"),\n",
    "    last(\"PRICE\", ignorenulls=True).alias(\"PRICE\"),\n",
    "    sum(col(\"QTY\")).alias(\"TOTAL_QTY\")\n",
    ").withColumn(\"sys_row_id\", monotonically_increasing_id())\n",
    "\n",
    "single_agg_final = aggtrans_single_agg.withColumn(\"ITEM_DESC\", col(\"ITEM_DESC\").cast(StringType())) \\\n",
    "    .withColumn(\"COUNT_ITEM\", lit(None).cast(LongType())) \\\n",
    "    .select(\n",
    "        col(\"STORE_ID\"),\n",
    "        col(\"ITEM_DESC\"),\n",
    "        col(\"QTY\"),\n",
    "        col(\"PRICE\"),\n",
    "        col(\"TOTAL_QTY\"),\n",
    "        col(\"COUNT_ITEM\"),\n",
    "        col(\"sys_row_id\")\n",
    "    )\n",
    "\n",
    "single_agg_final.write.format(\"delta\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"retail_agg_ff_single_agg\")\n",
    "\n",
    "# Aggregator: Nested Agg\n",
    "aggtrans_nested_agg = df_raw.groupBy(\"STORE_ID\", \"ITEM_DESC\").agg(\n",
    "    last(\"QTY\", ignorenulls=True).alias(\"QTY\"),\n",
    "    last(\"PRICE\", ignorenulls=True).alias(\"PRICE\"),\n",
    "    count(col(\"ITEM_DESC\")).alias(\"COUNT_ITEM\")\n",
    ").withColumn(\"sys_row_id\", monotonically_increasing_id())\n",
    "\n",
    "nested_agg_final = aggtrans_nested_agg.withColumn(\"ITEM_DESC\", col(\"ITEM_DESC\").cast(StringType())) \\\n",
    "    .withColumn(\"TOTAL_QTY\", lit(None).cast(LongType())) \\\n",
    "    .select(\n",
    "        col(\"STORE_ID\"),\n",
    "        col(\"ITEM_DESC\"),\n",
    "        col(\"QTY\"),\n",
    "        col(\"PRICE\"),\n",
    "        col(\"COUNT_ITEM\"),\n",
    "        col(\"TOTAL_QTY\"),\n",
    "        col(\"sys_row_id\")\n",
    "    )\n",
    "\n",
    "nested_agg_final.write.format(\"delta\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"retail_agg_ff_nested_agg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ef928e1-da3f-4cc4-a49e-a95b5ccb247a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Drop the single aggregation table if it exists\n",
    "spark.sql(\"DROP TABLE IF EXISTS retail_agg_ff_single_agg\")\n",
    "\n",
    "# Drop the nested aggregation table if it exists\n",
    "spark.sql(\"DROP TABLE IF EXISTS retail_agg_ff_nested_agg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "682c8472-d4ab-4399-bcf0-eef3d323abc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum as _sum, count as _count\n",
    "\n",
    "def validate_delta_tables(source_df, delta_total_df, delta_count_df):\n",
    "    print(\"==== STARTING RECONCILIATION CHECKS ====\\n Delta Total_Qty = Single Agg Table\\n Delta Count_Item = Nested Agg Table\")\n",
    "    \n",
    "    # CASE 1: Row Count Check\n",
    "    source_count = source_df.count()\n",
    "    total_count = delta_total_df.count()\n",
    "    count_item_count = delta_count_df.count()\n",
    "    \n",
    "    if total_count > 0:\n",
    "        print(f\"PASS: Delta Total_Qty table has {total_count} rows (source had {source_count})\")\n",
    "    else:\n",
    "        print(f\"FAIL: Delta Total_Qty table is empty!\")\n",
    "    \n",
    "    if count_item_count > 0:\n",
    "        print(f\"PASS: Delta Count_Item table has {count_item_count} rows (source had {source_count})\")\n",
    "    else:\n",
    "        print(f\"FAIL: Delta Count_Item table is empty!\")\n",
    "    \n",
    "    # CASE 2: Schema Check\n",
    "    expected_total_cols = ['STORE_ID','ITEM_DESC','QTY','PRICE', 'TOTAL_QTY','COUNT_ITEM','sys_row_id']\n",
    "    expected_count_cols = ['STORE_ID','ITEM_DESC','QTY','PRICE','COUNT_ITEM', 'TOTAL_QTY', 'sys_row_id']\n",
    "    \n",
    "    delta_total_cols = [c.lower() for c in delta_total_df.columns]\n",
    "    delta_count_cols = [c.lower() for c in delta_count_df.columns]\n",
    "    expected_total_cols_lower = [c.lower() for c in expected_total_cols]\n",
    "    expected_count_cols_lower = [c.lower() for c in expected_count_cols]\n",
    "    \n",
    "\n",
    "    if delta_total_cols == expected_total_cols_lower:\n",
    "        print(\"PASS: Delta Total_Qty schema matches expected columns (case-insensitive)\")\n",
    "    else:\n",
    "        print(f\"FAIL: Delta Total_Qty schema mismatch. Found: {delta_total_cols}\")\n",
    "        \n",
    "    if delta_count_cols == expected_count_cols_lower:\n",
    "        print(\"PASS: Delta Count_Item schema matches expected columns (case-insensitive)\")\n",
    "    else:\n",
    "        print(f\"FAIL: Delta Count_Item schema mismatch. Found: {delta_count_cols}\")\n",
    "    \n",
    "    # CASE 3: Aggregation correctness (Total_Qty)\n",
    "    agg_errors = 0\n",
    "    for row in source_df.select(\"Store_ID\",\"Item_Desc\").distinct().collect():\n",
    "        s_id = row['Store_ID']\n",
    "        item = row['Item_Desc']\n",
    "        source_sum = source_df.filter((col('Store_ID')==s_id) & (col('Item_Desc')==item)).agg(_sum('Qty')).collect()[0][0]\n",
    "        delta_sum = delta_total_df.filter((col('Store_ID')==s_id) & (col('Item_Desc')==item)).select('Total_Qty').collect()[0][0]\n",
    "        if source_sum == delta_sum:\n",
    "            print(f\"PASS: Total_Qty correct for Store {s_id}, Item {item} ({delta_sum})\")\n",
    "        else:\n",
    "            print(f\"FAIL: Total_Qty mismatch for Store {s_id}, Item {item}. Source={source_sum}, Delta={delta_sum}\")\n",
    "            agg_errors += 1\n",
    "    if agg_errors == 0:\n",
    "        print(\"PASS: All Total_Qty aggregation checks passed\")\n",
    "    else:\n",
    "        print(f\"FAIL: {agg_errors} Total_Qty aggregation errors found\")\n",
    "    \n",
    "    # CASE 4: Duplicate Check\n",
    "    dup_total = delta_total_df.groupBy('Store_ID','Item_Desc').count().filter(col('count')>1).count()\n",
    "    dup_count = delta_count_df.groupBy('Store_ID','Item_Desc').count().filter(col('count')>1).count()\n",
    "    \n",
    "    if dup_total == 0:\n",
    "        print(\"PASS: No duplicates in Delta Total_Qty table\")\n",
    "    else:\n",
    "        print(f\"FAIL: {dup_total} duplicate rows in Delta Total_Qty table\")\n",
    "    \n",
    "    if dup_count == 0:\n",
    "        print(\"PASS: No duplicates in Delta Count_Item table\")\n",
    "    else:\n",
    "        print(f\"FAIL: {dup_count} duplicate rows in Delta Count_Item table\")\n",
    "    \n",
    "    # CASE 5: Null / Invalid Values\n",
    "    null_total = delta_total_df.filter(col('Store_ID').isNull() | col('Item_Desc').isNull() | col('Qty').isNull() | col('Price').isNull()).count()\n",
    "    null_count = delta_count_df.filter(col('Store_ID').isNull() | col('Item_Desc').isNull() | col('Qty').isNull() | col('Price').isNull()).count()\n",
    "    \n",
    "    if null_total == 0:\n",
    "        print(\"PASS: No nulls in key columns of Delta Total_Qty table\")\n",
    "    else:\n",
    "        print(f\"FAIL: {null_total} null rows in Delta Total_Qty table\")\n",
    "    \n",
    "    if null_count == 0:\n",
    "        print(\"PASS: No nulls in key columns of Delta Count_Item table\")\n",
    "    else:\n",
    "        print(f\"FAIL: {null_count} null rows in Delta Count_Item table\")\n",
    "    \n",
    "    print(\"==== RECONCILIATION CHECKS COMPLETED ====\")\n",
    "    \n",
    "# Example usage:\n",
    "source_df = spark.read.table(\"retail_agg_ff\")\n",
    "delta_total_df = spark.read.table(\"retail_agg_ff_single_agg\")\n",
    "delta_count_df =  spark.read.table(\"retail_agg_ff_nested_agg\")\n",
    "\n",
    "validate_delta_tables(source_df, delta_total_df, delta_count_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dd46102-f703-4829-b618-d128096dec69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum as _sum, count as _count, countDistinct\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "def recon_case(case_id, description, condition, fail_msg, severity=\"FAIL\"):\n",
    "    if condition:\n",
    "        print(f\"❌ {severity} | {case_id} | {description}\")\n",
    "        print(f\"    ➜ {fail_msg}\\n\")\n",
    "    else:\n",
    "        print(f\"✅ PASS | {case_id} | {description}\\n\")\n",
    "\n",
    "def validate_pipeline(source_df, delta_single_df, delta_nested_df):\n",
    "\n",
    "    print(\"=\"*90)\n",
    "    print(\"STARTING FULL AUTOMATED RECONCILIATION\")\n",
    "    print(\"Single Agg = TOTAL_QTY table | Nested Agg = COUNT_ITEM table\")\n",
    "    print(\"=\"*90)\n",
    "\n",
    "    # -------------------------------\n",
    "    # CASE 1: Empty Load Check\n",
    "    # -------------------------------\n",
    "    recon_case(\n",
    "        \"TC_01\",\n",
    "        \"Delta tables should not be empty\",\n",
    "        delta_single_df.count() == 0 or delta_nested_df.count() == 0,\n",
    "        \"One or more Delta tables are empty\"\n",
    "    )\n",
    "\n",
    "    # -------------------------------\n",
    "    # CASE 2: Schema Validation\n",
    "    # -------------------------------\n",
    "    expected_single_cols = ['store_id','item_desc','qty','price','total_qty','count_item','sys_row_id']\n",
    "    expected_nested_cols = ['store_id','item_desc','qty','price','count_item','total_qty','sys_row_id']\n",
    "\n",
    "    recon_case(\n",
    "        \"TC_02\",\n",
    "        \"Single Agg schema validation\",\n",
    "        sorted([c.lower() for c in delta_single_df.columns]) != sorted(expected_single_cols),\n",
    "        f\"Expected {expected_single_cols}, Found {delta_single_df.columns}\"\n",
    "    )\n",
    "\n",
    "    recon_case(\n",
    "        \"TC_03\",\n",
    "        \"Nested Agg schema validation\",\n",
    "        sorted([c.lower() for c in delta_nested_df.columns]) != sorted(expected_nested_cols),\n",
    "        f\"Expected {expected_nested_cols}, Found {delta_nested_df.columns}\"\n",
    "    )\n",
    "\n",
    "    # -------------------------------\n",
    "    # CASE 3: Duplicate Detection\n",
    "    # -------------------------------\n",
    "    dup_single = delta_single_df.groupBy(\"Store_ID\",\"Item_Desc\").count().filter(col(\"count\") > 1).count()\n",
    "    dup_nested = delta_nested_df.groupBy(\"Store_ID\",\"Item_Desc\").count().filter(col(\"count\") > 1).count()\n",
    "\n",
    "    recon_case(\n",
    "        \"TC_04\",\n",
    "        \"Duplicate rows in Single Agg\",\n",
    "        dup_single > 0,\n",
    "        f\"{dup_single} duplicate Store_ID + Item_Desc rows found\"\n",
    "    )\n",
    "\n",
    "    recon_case(\n",
    "        \"TC_05\",\n",
    "        \"Duplicate rows in Nested Agg\",\n",
    "        dup_nested > 0,\n",
    "        f\"{dup_nested} duplicate Store_ID + Item_Desc rows found\"\n",
    "    )\n",
    "\n",
    "    # -------------------------------\n",
    "    # CASE 4: Null Critical Columns\n",
    "    # -------------------------------\n",
    "    null_single = delta_single_df.filter(\n",
    "        col(\"Store_ID\").isNull() |\n",
    "        col(\"Item_Desc\").isNull() |\n",
    "        col(\"Qty\").isNull() |\n",
    "        col(\"Price\").isNull()\n",
    "    ).count()\n",
    "\n",
    "    recon_case(\n",
    "        \"TC_06\",\n",
    "        \"Nulls in critical columns (Single Agg)\",\n",
    "        null_single > 0,\n",
    "        f\"{null_single} rows have NULL critical values\"\n",
    "    )\n",
    "\n",
    "    # -------------------------------\n",
    "    # CASE 5: Aggregation Integrity (TOTAL_QTY)\n",
    "    # -------------------------------\n",
    "    agg_mismatch = 0\n",
    "    for r in source_df.select(\"Store_ID\",\"Item_Desc\").distinct().collect():\n",
    "        s = r[\"Store_ID\"]\n",
    "        i = r[\"Item_Desc\"]\n",
    "\n",
    "        src_sum = source_df.filter(\n",
    "            (col(\"Store_ID\")==s) & (col(\"Item_Desc\")==i)\n",
    "        ).agg(_sum(\"Qty\")).collect()[0][0]\n",
    "\n",
    "        tgt_sum = delta_single_df.filter(\n",
    "            (col(\"Store_ID\")==s) & (col(\"Item_Desc\")==i)\n",
    "        ).select(\"Total_Qty\").collect()[0][0]\n",
    "\n",
    "        if src_sum != tgt_sum:\n",
    "            agg_mismatch += 1\n",
    "\n",
    "    recon_case(\n",
    "        \"TC_07\",\n",
    "        \"TOTAL_QTY aggregation correctness\",\n",
    "        agg_mismatch > 0,\n",
    "        f\"{agg_mismatch} Store/Item combinations have incorrect TOTAL_QTY\"\n",
    "    )\n",
    "\n",
    "    # -------------------------------\n",
    "    # CASE 6: COUNT_ITEM correctness (Nested Agg)\n",
    "    # -------------------------------\n",
    "    count_mismatch = 0\n",
    "    for r in source_df.select(\"Store_ID\",\"Item_Desc\").distinct().collect():\n",
    "        s = r[\"Store_ID\"]\n",
    "        i = r[\"Item_Desc\"]\n",
    "\n",
    "        src_cnt = source_df.filter(\n",
    "            (col(\"Store_ID\")==s) & (col(\"Item_Desc\")==i)\n",
    "        ).count()\n",
    "\n",
    "        tgt_cnt = delta_nested_df.filter(\n",
    "            (col(\"Store_ID\")==s) & (col(\"Item_Desc\")==i)\n",
    "        ).select(\"Count_Item\").collect()[0][0]\n",
    "\n",
    "        if src_cnt != tgt_cnt:\n",
    "            count_mismatch += 1\n",
    "\n",
    "    recon_case(\n",
    "        \"TC_08\",\n",
    "        \"COUNT_ITEM aggregation correctness\",\n",
    "        count_mismatch > 0,\n",
    "        f\"{count_mismatch} Store/Item combinations have incorrect COUNT_ITEM\"\n",
    "    )\n",
    "\n",
    "    # -------------------------------\n",
    "    # CASE 7: Completeness (Missing Stores)\n",
    "    # -------------------------------\n",
    "    missing_stores = source_df.select(\"Store_ID\").distinct() \\\n",
    "        .subtract(delta_single_df.select(\"Store_ID\").distinct()).count()\n",
    "\n",
    "    recon_case(\n",
    "        \"TC_09\",\n",
    "        \"All Store_IDs must exist in Delta\",\n",
    "        missing_stores > 0,\n",
    "        f\"{missing_stores} Store_IDs missing in Delta tables\"\n",
    "    )\n",
    "\n",
    "    # -------------------------------\n",
    "    # CASE 8: Qty Sanity Rule\n",
    "    # -------------------------------\n",
    "    invalid_qty = delta_single_df.filter(\n",
    "        (col(\"Qty\") <= 0) | (col(\"Qty\") > 1000) | (col(\"Qty\").cast(IntegerType()).isNull())\n",
    "    ).count()\n",
    "\n",
    "    recon_case(\n",
    "        \"TC_10\",\n",
    "        \"Qty sanity rule (1–1000, integer)\",\n",
    "        invalid_qty > 0,\n",
    "        f\"{invalid_qty} rows have invalid Qty values\",\n",
    "        severity=\"WARN\"\n",
    "    )\n",
    "\n",
    "    # -------------------------------\n",
    "    # CASE 9: Price Sanity Rule\n",
    "    # -------------------------------\n",
    "    invalid_price = delta_single_df.filter(\n",
    "        (col(\"Price\") <= 0) | (col(\"Price\") > 100)\n",
    "    ).count()\n",
    "\n",
    "    recon_case(\n",
    "        \"TC_11\",\n",
    "        \"Price sanity rule (0 < Price ≤ 100)\",\n",
    "        invalid_price > 0,\n",
    "        f\"{invalid_price} rows have unrealistic Price values\",\n",
    "        severity=\"WARN\"\n",
    "    )\n",
    "\n",
    "    print(\"=\"*90)\n",
    "    print(\"RECONCILIATION COMPLETED\")\n",
    "    print(\"=\"*90)\n",
    "\n",
    "\n",
    "source_df = spark.table(\"retail_agg_ff\")\n",
    "delta_single_df = spark.table(\"retail_agg_ff_single_agg\")\n",
    "delta_nested_df = spark.table(\"retail_agg_ff_nested_agg\")\n",
    "\n",
    "validate_pipeline(source_df, delta_single_df, delta_nested_df)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5166678967489421,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "retail_sales_aggregation_lakebridge_validation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
